{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uwhaxpl8PLJtOTU4Mdbo7BVVx20pgBeg","timestamp":1738083923074},{"file_id":"1f8mieip4FtZc9eckAN4F_bZhJLRW1gS7","timestamp":1738079466302},{"file_id":"1lj6B4EAkbLfhfOhDw3Y8N76jM8qgb8Ce","timestamp":1736438078436}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Vessel Scraping"],"metadata":{"id":"8WD7s9q1nJgH"}},{"cell_type":"markdown","source":["#### Globals"],"metadata":{"id":"AUWbYMaBnREa"}},{"cell_type":"code","source":["!apt-get update\n","!apt-get install -y chromium-chromedriver\n","!pip install selenium\n","!pip install beautifulsoup4"],"metadata":{"id":"pw2lwnxWxc9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from bs4 import BeautifulSoup\n","from tabulate import tabulate\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","from datetime import date\n","\n","import time\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import Select\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","\n","options = Options()\n"],"metadata":{"id":"7vVcH7Xszh_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Desired fields\n","final_fields = {\n","    \"Vessel Name\": None,\n","    \"IMO Number\": None,\n","    \"Flag\": None,\n","    \"MMSI\": None,\n","    \"Callsign\": None,\n","    \"Vessel Type / Fishing Method\": None,\n","    \"Registration Number\": None,\n","    \"Port of Registry\": None,\n","    \"Dates of Authorization\": None,\n","    \"Owner Name\": None,\n","    \"Owner Address\": None,\n","    \"Operator Name\": None,\n","    \"Operator Address\": None,\n","    \"Source of Information\": None,\n","    \"Source Link\": None,\n","    \"Date of Information\": None\n","}"],"metadata":{"id":"d5u_k5DU7Cxa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Equasis"],"metadata":{"id":"pzN2nZZz1Td-"}},{"cell_type":"code","source":["# Install required libraries\n","!pip install selenium beautifulsoup4 pandas lxml requests"],"metadata":{"id":"rdI692Q81aon"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scrape full vessel list from backend"],"metadata":{"id":"NOHExzfZI1SW"}},{"cell_type":"code","source":["\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from io import StringIO\n","\n","# Step 1: Set the URL and headers\n","url = \"https://www.equasis.org/EquasisWeb/restricted/Search?fs=Search\"\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n","}\n","\n","# Step 2: Add the active JSESSIONID cookie\n","cookies = {\n","    \"JSESSIONID\": \"\"  # Replace with your actual cookie\n","}\n","\n","MAX_PAGES = 15\n","\n","# Step 3: Define the form data template\n","form_data = {\n","    \"P_PAGE\": 1,\n","    \"P_PAGE_COMP\": 1,\n","    \"P_PAGE_SHIP\": 1,\n","    \"ongletActifSC\": \"ship\",\n","    \"P_ENTREE_HOME_HIDDEN\": \"\",\n","    \"P_IMO\": \"\",\n","    \"P_CALLSIGN\": \"\",\n","    \"P_NAME\": \"\",\n","    \"P_NAME_cu\": \"on\",\n","    \"P_MMSI\": \"\",\n","    \"P_GT_GT\": \"\",\n","    \"P_GT_LT\": \"\",\n","    \"P_DW_GT\": \"\",\n","    \"P_DW_LT\": \"\",\n","    \"P_YB_GT\": \"\",\n","    \"P_YB_LT\": \"\",\n","    \"P_CLASS_rb\": \"HC\",\n","    \"P_CLASS_ST_rb\": \"CM\",\n","    \"P_FLAG_rb\": \"HC\",\n","    \"P_CatTypeShip_rb\": \"CM\",\n","    \"P_CatTypeShip\": \"14\",\n","    \"P_CatTypeShip_p2\": \"14\",\n","    \"P_STATUS\": \"S\",\n","    \"buttonAdvancedSearch\": \"advancedOk\",\n","}\n","\n","all_dataframes = []\n","\n","for page in range(1, MAX_PAGES + 1):  # Adjust the range for the desired number of pages\n","    print(f\"Scraping page {page}...\")\n","    form_data[\"P_PAGE_SHIP\"] = page\n","    response = requests.post(url, headers=headers, cookies=cookies, data=form_data)\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, \"lxml\")\n","\n","        # Locate the table\n","        table = soup.find(\"table\")  # Adjust this to target the specific table if there are multiple\n","        if table:\n","            # Convert the HTML table to a pandas DataFrame\n","            df = pd.read_html(StringIO(str(table)))[0]\n","            all_dataframes.append(df)\n","        else:\n","            print(f\"Table not found on page {page}\")\n","    else:\n","        print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n","\n","final_dataframe = pd.concat(all_dataframes, ignore_index=True)\n","\n","\n","final_dataframe.to_csv(\"scraped_data.csv\", index=False)\n","print(\"Scraping completed. Data saved to scraped_data.csv.\")\n"],"metadata":{"id":"Dsqa8R_D1YO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First value in IMO # is actual number, set that\n","# After number, string until `Fish` is ship name\n","# Rest is Type of ship\n","testing_df = final_dataframe.copy()\n","columns = [\"IMO number\", \"Name of ship\", \"Gross tonnage\", \"Type of ship\", \"Year of build\", \"Flag\"]\n","for idx, row in testing_df.iterrows():\n","    if pd.isna(row[\"Gross tonnage\"]) and pd.isna(row[\"Flag\"]):\n","        # its a messed up row\n","        imo_num = row[\"IMO number\"].split(\" \")[0]\n","        testing_df.at[idx, \"IMO number\"] = imo_num\n","        rest = row[\"IMO number\"].split(\" \")[1:]\n","        rest = \" \".join(rest)\n","        testing_df.at[idx, \"Name of ship\"] = rest.split(\"Fish\")[0]\n","        testing_df.at[idx, \"Type of ship\"] = 'Fish' + rest.split(\"Fish\")[1]\n","\n","testing_df.to_csv(\"scraped_data_fixed.csv\", index=False)"],"metadata":{"id":"q232-gA7B-Uy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Convert dataframe to desired format"],"metadata":{"id":"pUQYWYd5IpK0"}},{"cell_type":"code","source":["new_df = pd.DataFrame(columns=final_fields)\n","\n","# Copy over matching columns\n","for idx, row in testing_df.iterrows():\n","    new_df.loc[idx, \"Vessel Name\"] = row[\"Name of ship\"]\n","    new_df.loc[idx, \"IMO Number\"] = row[\"IMO number\"]\n","    new_df.loc[idx, \"Flag\"] = row[\"Flag\"]\n","    new_df.loc[idx, \"Vessel Type / Fishing Method\"] = row[\"Type of ship\"]\n","    new_df.loc[idx, \"Source of Information\"] = \"Equasis\"\n","    new_df.loc[idx, \"Source Link\"] = \"https://www.equasis.org/EquasisWeb/restricted/Search?fs=Search\"\n","    new_df.loc[idx, \"Date of Information\"] = date.today()\n","new_df.head()"],"metadata":{"id":"_JgGbiw-Ir3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import from existing csv"],"metadata":{"id":"rjdJgyRJJEvF"}},{"cell_type":"code","source":["new_df = pd.read_csv(\"equasis-checkpoint.csv\")\n","new_df[\"Callsign\"] = new_df[\"Callsign\"].astype(object)\n","new_df[\"MMSI\"] = new_df[\"MMSI\"].astype(object)\n","new_df[\"Owner Address\"] = new_df[\"Owner Address\"].astype(object)\n","new_df[\"Owner Name\"] = new_df[\"Owner Name\"].astype(object)\n","new_df[\"Operator Name\"] = new_df[\"Operator Name\"].astype(object)\n","new_df[\"Operator Address\"] = new_df[\"Operator Address\"].astype(object)\n"],"metadata":{"id":"Gap0x6W3IohB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add data from individual vessel search"],"metadata":{"id":"bMOSiyQXGujL"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","# Base URL and headers\n","url = \"https://www.equasis.org/EquasisWeb/restricted/ShipInfo?fs=Search\"\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n","}\n","cookies = {\n","    \"JSESSIONID\": \"\",  # Replace with your actual cookie\n","}\n","\n","# Function to extract values based on the label\n","def extract_value(soup, label):\n","    # print (f\"Searching page: {soup.find('h4').get_text(strip=True)}\")\n","    label_b = soup.find(\"b\", string=lambda s: s and label in s)\n","    if label_b:\n","        # Get the next sibling <div> for the value\n","        value_div = label_b.find_parent(\"div\").find_next_sibling(\"div\")\n","        if value_div:\n","            return value_div.get_text(strip=True)\n","\n","def extract_owner_operator(soup):\n","    ship_info = {\"Owner Name\": '', 'Owner Address': '', \"Operator Name\": '', 'Operator Address': ''}\n","    for row in soup.find_all(\"tr\"):\n","      # If row is null, return empty\n","      if not row:\n","        return ship_info\n","      cells = row.find_all(\"td\")\n","      if len(cells) >= 4:  # Ensure there are enough columns\n","          role = cells[1].get_text(strip=True)\n","          company = cells[2].get_text(strip=True)\n","          address = cells[3].get_text(strip=True)\n","\n","          if role == \"Ship manager/Commercial manager\":\n","              ship_info[\"Owner Name\"] = company\n","              ship_info[\"Owner Address\"] = address\n","          elif role == \"Registered owner\":\n","              ship_info[\"Operator Name\"] = company\n","              ship_info[\"Operator Address\"] = address\n","    return ship_info\n","\n","def saveCheckpoint(df):\n","    df.to_csv(\"equasis-checkpoint.csv\", index=False)\n","    print(f\"Saved progress to equasis-checkpoint.csv\")\n","\n","MAX_RUN = 100 # must stay under 500 a day\n","WITH_PAUSES = True\n","LAST_IDX = 1420; # last checkpoint index populated with details\n","\n","# Setup dataframe from most recent run\n","new_df = pd.read_csv(\"equasis-checkpoint.csv\")\n","new_df[\"Callsign\"] = new_df[\"Callsign\"].astype(object)\n","new_df[\"MMSI\"] = new_df[\"MMSI\"].astype(object)\n","new_df[\"Owner Address\"] = new_df[\"Owner Address\"].astype(object)\n","new_df[\"Owner Name\"] = new_df[\"Owner Name\"].astype(object)\n","new_df[\"Operator Name\"] = new_df[\"Operator Name\"].astype(object)\n","new_df[\"Operator Address\"] = new_df[\"Operator Address\"].astype(object)\n","\n","full_df = new_df.copy()\n","\n","# Iterate over each IMO number in the DataFrame\n","count = 0\n","for index, row in full_df.iterrows():\n","    if index <= LAST_IDX:\n","        continue  # Skip already processed rows\n","\n","    # Every 10% save progress via csv export of current full_df\n","    if MAX_RUN >= 10 and count % 10 == 0 and count != 0:\n","        full_df.to_csv(\"equasis-progress.csv\", index=False)\n","        print(f\"Saved progress to equasis-progress.csv\")\n","        print (f\"Percent complete: {(count / MAX_RUN) * 100}%\")\n","        # Sleep for 30s every 10 requests\n","        if WITH_PAUSES:\n","            print(f\"Sleeping for 32 seconds...\")\n","            time.sleep(32)\n","\n","    # Skip this row if lacking IMO\n","    if pd.isna(row[\"IMO Number\"]):\n","        continue\n","\n","    form_data = {\n","        \"P_IMO\": row[\"IMO Number\"],  # Query parameter with IMO number\n","    }\n","\n","\n","    # Make the POST request\n","    response = requests.post(url, headers=headers, cookies=cookies, data=form_data)\n","    count += 1\n","    if response.status_code == 200:\n","        soup = BeautifulSoup(response.text, \"lxml\")\n","\n","        # Scrape Callsign and MMSI\n","        call_sign = extract_value(soup, \"Call Sign\")\n","        mmsi = extract_value(soup, \"MMSI\")\n","        if call_sign:\n","            full_df.at[index, \"Callsign\"] = call_sign\n","        if mmsi:\n","            full_df.at[index, \"MMSI\"] = mmsi\n","\n","        # Scrape owner/operator details\n","        owner_operator = extract_owner_operator(soup)\n","        for key, val in owner_operator.items():\n","            full_df.at[index, key] = val\n","\n","        if count > MAX_RUN:\n","          print(\"Hit max processing. Exiting. Ended at index: \" + str(index))\n","          break\n","\n","    else:\n","        print(f\"Failed to fetch data for IMO number {row['IMO Number']}. Status code: {response.status_code}\")\n","\n","full_df.to_csv(\"final.csv\", index=False)\n","print(\"Scraping completed. Data saved to final.csv.\")\n"],"metadata":{"id":"WmmxYJm2G6x1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## IOTC"],"metadata":{"id":"yidoJnsAaJAL"}},{"cell_type":"markdown","source":["### Initialize IOTC Mapping Data\n","e.g the codes they use `T-KHM` to the data we care about `Cambodia` , `KHM`"],"metadata":{"id":"HL2LirJPdk1M"}},{"cell_type":"code","source":["# Populate tenant ref map\n","import requests\n","import json\n","\n","# Endpoint URL\n","url = \"https://rav.iotc.org/domain/tenantrefs\"\n","\n","headers = {\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Make the GET request\n","response = requests.get(url, headers=headers)\n","\n","# Check the response status\n","if response.status_code == 200:\n","    # Parse the JSON response\n","    data = response.json()\n","\n","    print(f\"Total entries fetched: {len(data)}\")\n","else:\n","    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n","\n","# Convert to map of `id` : {code: '', name: ''}\n","iotc_tenant_mapping = {}\n","for tenant in data:\n","    iotc_tenant_mapping[tenant[\"id\"]] = {\"code\": tenant[\"code\"], \"name\": tenant[\"name\"][\"en\"]}"],"metadata":{"id":"ZqrSlekHcmDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Populate tag conversions\n","# Endpoint URL\n","url = \"https://rav.iotc.org/domain/tag\"\n","\n","headers = {\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Make the GET request\n","response = requests.get(url, headers=headers)\n","\n","# Check the response status\n","if response.status_code == 200:\n","    # Parse the JSON response\n","    data = response.json()\n","\n","    print(f\"Total entries fetched: {len(data)}\")\n","else:\n","    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n","\n","# Convert to map of `id` : {code: '', name: ''}\n","iotc_tag_mapping = {}\n","for tag in data:\n","    iotc_tag_mapping[tag[\"id\"]] = tag[\"name\"][\"en\"]\n","\n","iotc_tag_mapping[\"TG-lb-type\"]"],"metadata":{"id":"KWP7Zx-LgM7R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loop and collect data from their backend"],"metadata":{"id":"FTeELvzfd1q6"}},{"cell_type":"code","source":["def extract_fields(entry):\n","    return {\n","        \"Vessel Name\": entry.get(\"details\", {}).get(\"name\"),\n","        \"IMO Number\": next((id.split(\"TG-imo-scheme://\")[1] for id in entry.get(\"details\", {}).get(\"identifiers\", []) if \"TG-imo-scheme://\" in id), None),\n","        \"Flag\": iotc_tenant_mapping[entry.get(\"details\", {}).get(\"flagstate\")]['code'],\n","        \"MMSI\": None,\n","        \"Callsign\": next((id.split(\"TG-ircs-scheme://\")[1] for id in entry.get(\"details\", {}).get(\"identifiers\", []) if \"TG-ircs-scheme://\" in id), None),\n","        \"Vessel Type / Fishing Method\": iotc_tag_mapping[entry.get(\"details\", {}).get(\"vesselType\")],\n","        \"Registration Number\": next((id.split(\"TG-regno-scheme://\")[1] for id in entry.get(\"details\", {}).get(\"identifiers\", []) if \"TG-regno-scheme://\" in id), None),\n","        \"Port of Registry\": entry.get(\"details\", {}).get(\"port\", {}).get(\"name\"),\n","        \"Dates of Authorization\": f\"{entry.get('authorization', {}).get('from')} to {entry.get('authorization', {}).get('to')}\",\n","        \"Owner Name\": next((contact.get(\"name\") for contact in entry.get(\"details\", {}).get(\"contacts\", []) if contact.get(\"type\") == \"TG-owner-contact\"), None),\n","        \"Owner Address\": next((contact.get(\"address\") for contact in entry.get(\"details\", {}).get(\"contacts\", []) if contact.get(\"type\") == \"TG-owner-contact\"), None),\n","        \"Operator Name\": next((contact.get(\"name\") for contact in entry.get(\"details\", {}).get(\"contacts\", []) if contact.get(\"type\") == \"TG-operatorcontact\"), None),\n","        \"Operator Address\": next((contact.get(\"address\") for contact in entry.get(\"details\", {}).get(\"contacts\", []) if contact.get(\"type\") == \"TG-operatorcontact\"), None),\n","    }"],"metadata":{"id":"-rutQ70ze9J0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","import json\n","\n","# Endpoint URL\n","url = \"https://rav.iotc.org/domain/record/search\"\n","\n","# Base payload\n","payload_template = {\n","    \"mode\": \"current\",\n","    \"sort\": [\n","        {\n","            \"field\": \"timestamp\",\n","            \"mode\": \"desc\"\n","        }\n","    ],\n","    \"conditions\": [],\n","    \"includeDelisted\": False,\n","    \"cursor\": {\n","        \"page\": 1, # we'll increment this one\n","        \"pageSize\": 100\n","    },\n","    \"language\": \"en\"\n","}\n","\n","headers = {\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Maximum page to fetch\n","MAX_PAGES = 53\n","\n","# To collect all results\n","all_results = []\n","\n","# Loop through pages\n","for page in range(1, MAX_PAGES + 1):\n","    # Update the page number in the payload\n","    payload = payload_template.copy()\n","    payload[\"cursor\"][\"page\"] = page\n","\n","    # Make the POST request\n","    response = requests.post(url, headers=headers, json=payload)\n","\n","    # Check the response status\n","    if response.status_code == 200:\n","        data = response.json()\n","        all_results.extend(data.get(\"results\", []))  # Assuming results are in a \"results\" key\n","        print(f\"Page {page} fetched successfully.\")\n","    else:\n","        print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n","        break\n","\n","# Display the total number of results fetched\n","print(f\"Total results fetched: {len(all_results)}\")\n","\n","print(\"Converting to dataframe...\")\n","df = pd.DataFrame([extract_fields(entry) for entry in all_results])\n","df.to_csv(\"iotc-data.csv\", index=False)\n"],"metadata":{"id":"guzbEm59aL_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9FWTMjv8VMVR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GFCM"],"metadata":{"id":"oFb_w6NM_Idj"}},{"cell_type":"code","source":["index_to_fields = {\n","    0: \"Flag\",\n","    1: \"Vessel Name\",\n","    2: \"Registration Number\",\n","    3: \"MMSI\",\n","    4: \"IMO Number\",\n","    5: \"Callsign\", # assumes \"IRCS\" maps to \"callsign\"\n","    6: None,  # skip licence indicator\n","    7: None,  # skip operational status\n","    8: None,  # skip LOA\n","    9: None,  # skip GT\n","    10: None, # skip Engine Power\n","    11: \"Vessel Type / Fishing Method\"\n","}\n","\n","def processVesselRow(row) :\n","  cells = row.find_elements(By.CSS_SELECTOR, \"div[role='gridcell']\")\n","  if (len(cells) == 0):\n","    print (\"Empty row\")\n","    return None\n","  # Skip vessels with operational status \"No\"\n","  if (cells[7] == \"No\"):\n","    return None\n","\n","  # Extract text from each cell\n","  cell_texts = [c.text.strip() for c in cells]\n","\n","  # Now, cell_texts[0] might be \"Select Row\".\n","  # We'll remove that if it's there:\n","  if cell_texts and \"Select Row\" in cell_texts[0]:\n","      cell_texts = cell_texts[1:]  # skip it\n","\n","  # Now we map them into a new dictionary for this vessel\n","  row_fields = final_fields.copy()\n","\n","  for i, val in enumerate(cell_texts):\n","      dict_key = index_to_fields.get(i)\n","      if dict_key:\n","          row_fields[dict_key] = val or None  # if val = '', store None\n","\n","  # (Optionally) fill in more fields if you have them\n","  row_fields[\"Source of Information\"] = \"GFCM\"\n","  row_fields[\"Source Link\"] = \"https://www.fao.org/gfcm/data/fleet/avl\"\n","  row_fields[\"Date of Information\"] = date.today()\n","\n","  return row_fields"],"metadata":{"id":"V0UyxytX6T-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["URL = \"https://www.fao.org/gfcm/data/fleet/avl\"\n","MAX_WAIT = 15\n","MAX_ENTRIES = 30\n","\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome(options=options)\n","\n","wait = WebDriverWait(driver, MAX_WAIT)\n","\n","try:\n","    driver.get(URL)\n","\n","    iframe_elem = wait.until(\n","    EC.presence_of_element_located((By.CSS_SELECTOR, \"iframe[src*='app.powerbi.com/view']\")))\n","\n","    driver.switch_to.frame(iframe_elem)\n","\n","    wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"scrollable-cells-container\")))\n","    scroll_container = driver.find_element(By.CLASS_NAME, \"scrollable-cells-container\")\n","\n","    last_count = 0\n","    while True:\n","        # Find all rows so far\n","        rows = driver.find_elements(By.CSS_SELECTOR, \"div[role='row']\")\n","        current_count = len(rows)\n","        print(f\"Currently have {current_count} rows.\")\n","\n","        # Scroll down to trigger loading the next set of rows\n","        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scroll_container)\n","        # Give the page a moment to load new rows if any\n","        time.sleep(8)\n","\n","        # Check if new rows were added\n","        if current_count == last_count:\n","            # No change in row count => probably at the end\n","            print(\"No new rows loaded. Finished scrolling.\")\n","            break\n","\n","        last_count = current_count\n","\n","    rows = driver.find_elements(By.CLASS_NAME, \"scrollable-cells-container\")\n","\n","    all_vessels = []\n","    n = len(rows)\n","\n","    count = 0\n","    for row in rows:\n","        count += 1\n","        vessel_data = processVesselRow(row)\n","        if vessel_data:\n","            all_vessels.append(vessel_data)\n","        if count % 10 : print (f\"Percent complete: {(count / n) * 100}%\")\n","    df = pd.DataFrame(all_vessels)\n","finally:\n","    driver.quit()\n","\n","df.head()\n","df.to_csv(\"gfcm-data.csv\", index=False)\n"],"metadata":{"id":"h0Sij5cLxy5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CCAMLR\n"],"metadata":{"id":"tB0V7Qz6xhcR"}},{"cell_type":"code","source":["BASE_URL = \"https://www.ccamlr.org\"\n","\n","def extractTables(soup):\n","    rows = soup.find_all('tr')\n","    data_list = []\n","\n","    for row in rows:\n","        cells = row.find_all('td')\n","        if not cells:\n","            continue\n","\n","        # --- Extract Ship Name and URL ---\n","        ship_td = row.find('td', class_='views-field-title-1')\n","        if ship_td:\n","            link = ship_td.find('a', href=True)\n","            if link:\n","                ship_name = link.get_text(strip=True)\n","                relative_url = link['href']\n","                full_url = BASE_URL + relative_url  # convert relative to absolute\n","            else:\n","                ship_name = \"\"\n","                full_url = \"\"\n","        else:\n","            ship_name = \"\"\n","            full_url = \"\"\n","\n","        # --- Extract Date Range ---\n","        period_td = row.find('td', class_='views-field-field-licence-period')\n","        if period_td:\n","            start_span = period_td.find('span', class_='date-display-start')\n","            end_span = period_td.find('span', class_='date-display-end')\n","            if start_span and end_span:\n","                start_date = start_span.get_text(strip=True)\n","                end_date = end_span.get_text(strip=True)\n","                date_range = f\"{start_date} to {end_date}\"\n","            else:\n","                date_range = \"\"\n","        else:\n","            date_range = \"\"\n","\n","        if ship_name or date_range:\n","            data_list.append({\n","                \"Ship Name\": ship_name,\n","                \"Ship URL\": full_url,\n","                \"Date Range\": date_range\n","            })\n","\n","    df = pd.DataFrame(data_list, columns=[\"Ship Name\", \"Ship URL\", \"Date Range\"])\n","    return df"],"metadata":{"id":"-Q-TRjjWUSJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.support.ui import Select\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","\n","URL = \"https://www.ccamlr.org/en/compliance/list-vessel-authorisations\"\n","MAX_WAIT = 15\n","MAX_TABLES = 1\n","\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome(options=options)\n","\n","wait = WebDriverWait(driver, MAX_WAIT)\n","\n","try:\n","    driver.get(URL)\n","\n","    element = wait.until(EC.visibility_of_element_located((By.ID, \"edit-ccamlr-season-ccamlr-season-override\")))\n","\n","    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","\n","    all_data = extractTables(soup)\n","\n","\n","finally:\n","    driver.quit()"],"metadata":{"id":"NZ4A-7VA0HzH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_data.to_csv(\"ships.csv\", index=False)"],"metadata":{"id":"PyUt_zB2VDCM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prune to latest entry\n","df = all_data\n","\n","# 2) Parse 'Date Range' into separate start/end columns\n","#    Splitting on \" to \" will give us two strings: e.g. \"01 Dec 2024\" and \"30 Nov 2025\".\n","df[['start_str', 'end_str']] = df['Date Range'].str.split(' to ', expand=True)\n","\n","# 3) Convert to datetime objects\n","#    Adjust the format string (%d %b %Y) if the date format is different on your site.\n","df['start_date'] = pd.to_datetime(df['start_str'], format='%d %b %Y')\n","df['end_date'] = pd.to_datetime(df['end_str'], format='%d %b %Y')\n","\n","# 4) Sort by end_date descending, then start_date descending\n","df_sorted = df.sort_values(by=['end_date', 'start_date'], ascending=[False, False])\n","\n","# 5) For each ship, keep only the row with the \"max\" end_date and then start_date\n","#    groupby('Ship Name') + head(1) will keep the first row in each group\n","#    (which is the max after our descending sort).\n","df_unique = df_sorted.groupby('Ship Name', as_index=False).head(1)\n","\n","# (Optional) Drop the temporary columns\n","df_unique.drop(columns=['start_str', 'end_str', 'start_date', 'end_date'], inplace=True)\n","\n","# 6) Output the pruned data to CSV\n","df_unique.to_csv('ships_pruned.csv', index=False)"],"metadata":{"id":"seFfBmnlXVmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime\n","import re\n","\n","def scrapeVesselPage(shipName, shipUrl):\n","  driver = webdriver.Chrome(options=options)\n","  wait = WebDriverWait(driver, 10)\n","\n","  driver.get(shipUrl)\n","\n","  try:\n","      # Wait until vessel content present\n","      vessel_div_element = wait.until(\n","          EC.presence_of_element_located((By.CSS_SELECTOR, \"div.vessel-content\"))\n","      )\n","\n","      # Get last modified from 'meta submitted' class\n","      dateOfInfo = None\n","      lastModField = driver.find_element(By.CLASS_NAME, \"meta.submitted\")\n","      if (lastModField):\n","          lastModDate = lastModField.text\n","          pattern = r\"This page was last modified on (\\d{1,2} [A-Za-z]{3} \\d{4})\"\n","          matchText = re.search(pattern, lastModDate)\n","          if matchText:\n","            dateOfInfo = matchText.group(1)\n","\n","      html = driver.page_source\n","\n","  finally:\n","      driver.quit()\n","\n","  soup = BeautifulSoup(html, \"html.parser\")\n","\n","  vessel_div = soup.select_one(\"div.vessel-content\")\n","  if not vessel_div:\n","      print(\"Could not find the vessel-content div.\")\n","      vessel_text = \"\"\n","  else:\n","      vessel_text = vessel_div.get_text(\"\\n\", strip=True)\n","\n","  fields = {\n","      \"IMO Number\": None,\n","      \"Vessel Name\": None,\n","      \"Flag\": None,\n","      \"MMSI\": None,\n","      \"Callsign\": None,\n","      \"Vessel Type / Fishing Method\": None,\n","      \"Registration Number\": None,\n","      \"Port of Registry\": None,\n","      \"Dates of Authorization\": None,\n","      \"Owner Name\": None,\n","      \"Owner Address\": None,\n","      \"Operator Name\": None,\n","      \"Operator Address\": None,\n","      \"Source of Information\": None,\n","      \"Source Link\": None,\n","      \"Date of Information\": None\n","  }\n","\n","  lines = vessel_text.splitlines()\n","  lines = [l.strip() for l in lines if l.strip()]  # remove empty lines\n","\n","  fields_raw = {}\n","  current_label = None\n","  current_value = []\n","\n","  def store_current():\n","      \"\"\"Store the accumulated value into fields_raw under current_label.\"\"\"\n","      if current_label is not None:\n","          # Join all lines for this label into one string\n","          # e.g., [\"16 Nov 2021\", \"to\", \"28 Feb 2022\"] -> \"16 Nov 2021 to 28 Feb 2022\"\n","          joined_value = ' '.join(current_value).strip()\n","          fields_raw[current_label] = joined_value\n","\n","  for line in lines:\n","      # If line ends with a colon, treat it as a new label\n","      if re.match(r'.+:$', line):\n","          # If we were accumulating a previous label/value, store it\n","          store_current()\n","          # Start a new label\n","          current_label = line[:-1].strip()  # remove the colon at the end\n","          current_value = []\n","      else:\n","          # It's part of the value for the current label\n","          current_value.append(line)\n","\n","  # Store the last accumulated label/value\n","  store_current()\n","\n","  vessel_data = {\n","      \"Vessel Name\": shipName,\n","      \"IMO Number\": fields_raw.get(\"IMO Number\", None),\n","      \"Flag\": fields_raw.get(\"Flag\", None),\n","      \"MMSI\": None,\n","      \"Callsign\": fields_raw.get(\"Callsign\", None),\n","      \"Vessel Type / Fishing Method\": None,\n","      \"Registration Number\": fields_raw.get(\"Registration Number\", None),\n","      \"Port of Registry\": fields_raw.get(\"Port of Registry\", None),\n","      \"Dates of Authorization\": fields_raw.get(\"Effective Date\", None),\n","      \"Owner Name\": fields_raw.get(\"Owner\", None),\n","      \"Owner Address\": None,\n","      \"Operator Name\": fields_raw.get(\"Operator\", None),\n","      \"Operator Address\": None,\n","      \"Source of Information\": \"CCAMLR\",\n","      \"Source Link\": shipUrl,\n","      \"Date of Information\": dateOfInfo\n","  }\n","\n","\n","  vessel_data\n","\n","  return vessel_data"],"metadata":{"id":"Y6j3H_SV39kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_list = []\n","for index, row in df_unique.iterrows():\n","    vessel_url = row[\"Ship URL\"]\n","    vessel_name = row[\"Ship Name\"]\n","    data_list.append(scrapeVesselPage(vessel_name, vessel_url))\n","\n","ships_enhanced = pd.DataFrame(data_list)"],"metadata":{"id":"zKMn-Ddeai84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import date"],"metadata":{"id":"HKUBYB-isGMd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ships_enhanced['Date of Information'] = date.today()\n","ships_enhanced.to_csv('ships_enhanced.csv', index=False)"],"metadata":{"id":"VQTRuAgub-nG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WCPFC\n","- [AX]: `Vessel Authorization Link for The Western and Central Pacific Fisheries Commission`\n","  - Use the specific vessel page, found after clicking the search result, e.g. https://vessels.wcpfc.int/vessel/11327\n","- [AY]: `Date Source Info Last Updated for The Western and Central Pacific Fisheries Commission`\n","  - Use the *Last Update* field on the specific vessel page, https://vessels.wcpfc.int/vessel/11327\n","- [AZ]: `Date Source Info Acquired for The Western and Central Pacific Fisheries Commission`\n","\n"],"metadata":{"id":"nYt6q9unpD-J"}},{"cell_type":"code","source":["# First get the auth link via the main page's table\n","# Set it per name & ircs\n","import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","# Base URL and page range\n","MAX_PAGE = 62\n","base_url = \"https://vessels.wcpfc.int/browse-rfv\"\n","page_range = range(1, MAX_PAGE)  # Pages 1 to 61\n","\n","# Output file\n","output_file = \"wcpfc_phase2.csv\"\n","\n","# Prepare CSV file\n","with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n","    writer = csv.writer(file)\n","    writer.writerow([\"Vessel Name\", \"IRCS\", \"VID\"])  # Header row\n","\n","    # Loop through all pages\n","    for page in page_range:\n","        print(f\"Scraping page {page}...\")\n","\n","        # Construct the URL for the current page\n","        url = f\"{base_url}?page={page}\" if page > 1 else base_url\n","\n","        # Send GET request\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise error if request fails\n","\n","        # Parse HTML content\n","        soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","        # Find all rows in the table body\n","        rows = soup.find_all(\"tr\")\n","\n","        for row in rows:\n","            # Extract the vessel name, IRCS, and VID\n","            vessel_name_cell = row.find(\"td\", class_=\"views-field-vsl-vessel-name\")\n","            ircs_cell = row.find(\"td\", class_=\"views-field-vsl-ircs\")\n","            vid_cell = row.find(\"td\", class_=\"views-field-vsl-vslo-vessel-id\")\n","\n","            if vessel_name_cell and ircs_cell and vid_cell:\n","                vessel_name = vessel_name_cell.text.strip()\n","                ircs = ircs_cell.text.strip()\n","                vid = vid_cell.text.strip()\n","\n","                # Write the data to the CSV file\n","                writer.writerow([vessel_name, ircs, vid])\n","\n","print(f\"Data scraping completed. Saved to {output_file}.\")\n"],"metadata":{"id":"xyHBqFxUpLSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Visit Each Page and Nab Last Updated Date"],"metadata":{"id":"ZOMX3w9n0pak"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# Read in the previous CSV as a dataframe\n","input_csv = \"wcpfc_phase2.csv\"\n","output_csv = \"vessel_data_with_dates.csv\"\n","df = pd.read_csv(input_csv)\n","\n","# Base URL for vessel pages\n","base_url = \"https://vessels.wcpfc.int/vessel/\"\n","\n","# Add a new column to store the \"Last Updated\" date\n","df[\"Last Updated\"] = None\n","\n","# Loop through each row in the dataframe\n","for index, row in df.iterrows():\n","    vid = row[\"VID\"]\n","    vessel_url = f\"{base_url}{vid}\"\n","\n","    # print(f\"Scraping VID {vid}...\")\n","    if (index % 10 == 0): print(f\"Percent complete: {(index / len(df)) * 100}%\" )\n","\n","    try:\n","        # Send GET request to the vessel page\n","        response = requests.get(vessel_url)\n","        response.raise_for_status()  # Raise error if request fails\n","\n","        # Parse the HTML content\n","        soup = BeautifulSoup(response.text, \"html.parser\")\n","\n","        # Extract the \"Last Updated\" date\n","        last_updated_div = soup.find(\"div\", class_=\"vessel-version__vsl-changed\")\n","        if last_updated_div:\n","            last_updated_date = last_updated_div.find(\"time\")[\"datetime\"]\n","            # print(f\"VID: {vid}, Last Updated: {last_updated_date}\")\n","        else:\n","            last_updated_date = \"\"\n","\n","        # Update the dataframe with the scraped date\n","        df.at[index, \"Last Updated\"] = last_updated_date\n","\n","    except Exception as e:\n","        print(f\"Error scraping VID {vid}: {e}\")\n","        df.at[index, \"Last Updated\"] = \"Error\"\n","\n","    # Optional: Delay to avoid overwhelming the server\n","    # time.sleep(.2)\n","\n","# Save the updated dataframe to a new CSV file\n","df.to_csv(output_csv, index=False)\n","\n","print(f\"Scraping completed. Updated data saved to {output_csv}.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zz7008CX0vj1","executionInfo":{"status":"ok","timestamp":1737754383052,"user_tz":300,"elapsed":2510905,"user":{"displayName":"Ben","userId":"17439410891067034258"}},"outputId":"5850f4a1-25e4-44ce-9f94-77b2cb3fc6f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Percent complete: 0.0%\n","Percent complete: 0.3304692663582287%\n","Percent complete: 0.6609385327164574%\n","Percent complete: 0.991407799074686%\n","Percent complete: 1.3218770654329148%\n","Percent complete: 1.6523463317911435%\n","Percent complete: 1.982815598149372%\n","Percent complete: 2.313284864507601%\n","Percent complete: 2.6437541308658297%\n","Percent complete: 2.9742233972240584%\n","Percent complete: 3.304692663582287%\n","Percent complete: 3.6351619299405153%\n","Percent complete: 3.965631196298744%\n","Percent complete: 4.296100462656973%\n","Percent complete: 4.626569729015202%\n","Percent complete: 4.95703899537343%\n","Percent complete: 5.287508261731659%\n","Percent complete: 5.617977528089887%\n","Percent complete: 5.948446794448117%\n","Percent complete: 6.278916060806345%\n","Percent complete: 6.609385327164574%\n","Percent complete: 6.939854593522803%\n","Percent complete: 7.270323859881031%\n","Percent complete: 7.600793126239259%\n","Percent complete: 7.931262392597488%\n","Percent complete: 8.261731658955718%\n","Percent complete: 8.592200925313946%\n","Percent complete: 8.922670191672175%\n","Percent complete: 9.253139458030404%\n","Percent complete: 9.58360872438863%\n","Percent complete: 9.91407799074686%\n","Percent complete: 10.24454725710509%\n","Percent complete: 10.575016523463319%\n","Percent complete: 10.905485789821547%\n","Percent complete: 11.235955056179774%\n","Percent complete: 11.566424322538003%\n","Percent complete: 11.896893588896233%\n","Percent complete: 12.227362855254462%\n","Percent complete: 12.55783212161269%\n","Percent complete: 12.888301387970918%\n","Percent complete: 13.218770654329148%\n","Percent complete: 13.549239920687375%\n","Percent complete: 13.879709187045606%\n","Percent complete: 14.210178453403833%\n","Percent complete: 14.540647719762061%\n","Percent complete: 14.871116986120292%\n","Percent complete: 15.201586252478519%\n","Percent complete: 15.53205551883675%\n","Percent complete: 15.862524785194976%\n","Percent complete: 16.192994051553207%\n","Percent complete: 16.523463317911435%\n","Percent complete: 16.853932584269664%\n","Percent complete: 17.184401850627893%\n","Percent complete: 17.514871116986118%\n","Percent complete: 17.84534038334435%\n","Percent complete: 18.17580964970258%\n","Percent complete: 18.506278916060808%\n","Percent complete: 18.836748182419036%\n","Percent complete: 19.16721744877726%\n","Percent complete: 19.497686715135494%\n","Percent complete: 19.82815598149372%\n","Percent complete: 20.15862524785195%\n","Percent complete: 20.48909451421018%\n","Percent complete: 20.819563780568405%\n","Percent complete: 21.150033046926637%\n","Percent complete: 21.480502313284862%\n","Percent complete: 21.810971579643095%\n","Percent complete: 22.141440846001323%\n","Percent complete: 22.47191011235955%\n","Percent complete: 22.80237937871778%\n","Percent complete: 23.132848645076006%\n","Percent complete: 23.46331791143424%\n","Percent complete: 23.793787177792467%\n","Percent complete: 24.124256444150692%\n","Percent complete: 24.454725710508924%\n","Percent complete: 24.78519497686715%\n","Percent complete: 25.11566424322538%\n","Percent complete: 25.446133509583607%\n","Percent complete: 25.776602775941836%\n","Percent complete: 26.107072042300068%\n","Percent complete: 26.437541308658297%\n","Percent complete: 26.768010575016522%\n","Percent complete: 27.09847984137475%\n","Percent complete: 27.42894910773298%\n","Percent complete: 27.75941837409121%\n","Percent complete: 28.08988764044944%\n","Percent complete: 28.420356906807665%\n","Percent complete: 28.750826173165894%\n","Percent complete: 29.081295439524123%\n","Percent complete: 29.411764705882355%\n","Percent complete: 29.742233972240584%\n","Percent complete: 30.07270323859881%\n","Percent complete: 30.403172504957038%\n","Percent complete: 30.733641771315266%\n","Percent complete: 31.0641110376735%\n","Percent complete: 31.394580304031727%\n","Percent complete: 31.725049570389952%\n","Percent complete: 32.055518836748185%\n","Percent complete: 32.38598810310641%\n","Percent complete: 32.71645736946464%\n","Percent complete: 33.04692663582287%\n","Percent complete: 33.37739590218109%\n","Percent complete: 33.70786516853933%\n","Percent complete: 34.03833443489756%\n","Percent complete: 34.368803701255786%\n","Percent complete: 34.699272967614014%\n","Percent complete: 35.029742233972236%\n","Percent complete: 35.36021150033047%\n","Percent complete: 35.6906807666887%\n","Percent complete: 36.02115003304693%\n","Percent complete: 36.35161929940516%\n","Percent complete: 36.68208856576338%\n","Percent complete: 37.012557832121615%\n","Percent complete: 37.343027098479844%\n","Percent complete: 37.67349636483807%\n","Percent complete: 38.0039656311963%\n","Percent complete: 38.33443489755452%\n","Percent complete: 38.66490416391276%\n","Percent complete: 38.99537343027099%\n","Percent complete: 39.325842696629216%\n","Percent complete: 39.65631196298744%\n","Percent complete: 39.98678122934567%\n","Percent complete: 40.3172504957039%\n","Percent complete: 40.64771976206213%\n","Percent complete: 40.97818902842036%\n","Percent complete: 41.30865829477858%\n","Percent complete: 41.63912756113681%\n","Percent complete: 41.969596827495046%\n","Percent complete: 42.300066093853275%\n","Percent complete: 42.6305353602115%\n","Percent complete: 42.961004626569725%\n","Percent complete: 43.291473892927954%\n","Percent complete: 43.62194315928619%\n","Percent complete: 43.95241242564442%\n","Percent complete: 44.28288169200265%\n","Percent complete: 44.61335095836087%\n","Percent complete: 44.9438202247191%\n","Percent complete: 45.27428949107733%\n","Percent complete: 45.60475875743556%\n","Percent complete: 45.93522802379379%\n","Percent complete: 46.26569729015201%\n","Percent complete: 46.59616655651024%\n","Percent complete: 46.92663582286848%\n","Percent complete: 47.257105089226705%\n","Percent complete: 47.587574355584934%\n","Percent complete: 47.918043621943156%\n","Percent complete: 48.248512888301384%\n","Percent complete: 48.57898215465962%\n","Percent complete: 48.90945142101785%\n","Percent complete: 49.23992068737608%\n","Percent complete: 49.5703899537343%\n","Percent complete: 49.90085922009253%\n","Percent complete: 50.23132848645076%\n","Percent complete: 50.56179775280899%\n","Percent complete: 50.892267019167214%\n","Percent complete: 51.22273628552545%\n","Percent complete: 51.55320555188367%\n","Percent complete: 51.8836748182419%\n","Percent complete: 52.214144084600136%\n","Percent complete: 52.54461335095836%\n","Percent complete: 52.87508261731659%\n","Percent complete: 53.205551883674815%\n","Percent complete: 53.536021150033044%\n","Percent complete: 53.86649041639128%\n","Percent complete: 54.1969596827495%\n","Percent complete: 54.52742894910774%\n","Percent complete: 54.85789821546596%\n","Percent complete: 55.18836748182419%\n","Percent complete: 55.51883674818242%\n","Percent complete: 55.849306014540645%\n","Percent complete: 56.17977528089888%\n","Percent complete: 56.5102445472571%\n","Percent complete: 56.84071381361533%\n","Percent complete: 57.17118307997357%\n","Percent complete: 57.50165234633179%\n","Percent complete: 57.832121612690024%\n","Percent complete: 58.162590879048246%\n","Percent complete: 58.493060145406474%\n","Percent complete: 58.82352941176471%\n","Percent complete: 59.15399867812293%\n","Percent complete: 59.48446794448117%\n","Percent complete: 59.81493721083939%\n","Percent complete: 60.14540647719762%\n","Percent complete: 60.475875743555854%\n","Percent complete: 60.806345009914075%\n","Percent complete: 61.13681427627231%\n","Percent complete: 61.46728354263053%\n","Percent complete: 61.79775280898876%\n","Percent complete: 62.128222075347%\n","Percent complete: 62.45869134170522%\n","Percent complete: 62.789160608063455%\n","Percent complete: 63.119629874421676%\n","Percent complete: 63.450099140779905%\n","Percent complete: 63.78056840713814%\n","Percent complete: 64.11103767349637%\n","Percent complete: 64.4415069398546%\n","Percent complete: 64.77197620621283%\n","Percent complete: 65.10244547257105%\n","Percent complete: 65.43291473892928%\n","Percent complete: 65.7633840052875%\n","Percent complete: 66.09385327164574%\n","Percent complete: 66.42432253800396%\n","Percent complete: 66.75479180436218%\n","Percent complete: 67.08526107072042%\n","Percent complete: 67.41573033707866%\n","Percent complete: 67.74619960343688%\n","Percent complete: 68.07666886979511%\n","Percent complete: 68.40713813615334%\n","Percent complete: 68.73760740251157%\n","Percent complete: 69.0680766688698%\n","Percent complete: 69.39854593522803%\n","Percent complete: 69.72901520158625%\n","Percent complete: 70.05948446794447%\n","Percent complete: 70.38995373430271%\n","Percent complete: 70.72042300066094%\n","Percent complete: 71.05089226701918%\n","Percent complete: 71.3813615333774%\n","Percent complete: 71.71183079973562%\n","Percent complete: 72.04230006609386%\n","Percent complete: 72.37276933245208%\n","Percent complete: 72.70323859881032%\n","Percent complete: 73.03370786516854%\n","Percent complete: 73.36417713152676%\n","Percent complete: 73.694646397885%\n","Percent complete: 74.02511566424323%\n","Percent complete: 74.35558493060145%\n","Percent complete: 74.68605419695969%\n","Percent complete: 75.01652346331791%\n","Percent complete: 75.34699272967615%\n","Percent complete: 75.67746199603437%\n","Percent complete: 76.0079312623926%\n","Percent complete: 76.33840052875082%\n","Percent complete: 76.66886979510905%\n","Percent complete: 76.99933906146728%\n","Percent complete: 77.32980832782552%\n","Percent complete: 77.66027759418375%\n","Percent complete: 77.99074686054198%\n","Percent complete: 78.3212161269002%\n","Percent complete: 78.65168539325843%\n","Percent complete: 78.98215465961665%\n","Percent complete: 79.31262392597488%\n","Percent complete: 79.64309319233311%\n","Percent complete: 79.97356245869133%\n","Percent complete: 80.30403172504957%\n","Percent complete: 80.6345009914078%\n","Percent complete: 80.96497025776603%\n","Percent complete: 81.29543952412426%\n","Percent complete: 81.62590879048248%\n","Percent complete: 81.95637805684072%\n","Percent complete: 82.28684732319894%\n","Percent complete: 82.61731658955716%\n","Percent complete: 82.9477858559154%\n","Percent complete: 83.27825512227362%\n","Percent complete: 83.60872438863186%\n","Percent complete: 83.93919365499009%\n","Percent complete: 84.26966292134831%\n","Percent complete: 84.60013218770655%\n","Percent complete: 84.93060145406477%\n","Percent complete: 85.261070720423%\n","Percent complete: 85.59153998678123%\n","Percent complete: 85.92200925313945%\n","Percent complete: 86.25247851949769%\n","Percent complete: 86.58294778585591%\n","Percent complete: 86.91341705221414%\n","Percent complete: 87.24388631857238%\n","Percent complete: 87.5743555849306%\n","Percent complete: 87.90482485128884%\n","Percent complete: 88.23529411764706%\n","Percent complete: 88.5657633840053%\n","Percent complete: 88.89623265036352%\n","Percent complete: 89.22670191672174%\n","Percent complete: 89.55717118307997%\n","Percent complete: 89.8876404494382%\n","Percent complete: 90.21810971579643%\n","Percent complete: 90.54857898215467%\n","Percent complete: 90.87904824851289%\n","Percent complete: 91.20951751487112%\n","Percent complete: 91.53998678122935%\n","Percent complete: 91.87045604758758%\n","Percent complete: 92.2009253139458%\n","Percent complete: 92.53139458030402%\n","Percent complete: 92.86186384666226%\n","Percent complete: 93.19233311302048%\n","Percent complete: 93.52280237937872%\n","Percent complete: 93.85327164573695%\n","Percent complete: 94.18374091209517%\n","Percent complete: 94.51421017845341%\n","Percent complete: 94.84467944481163%\n","Percent complete: 95.17514871116987%\n","Percent complete: 95.50561797752809%\n","Percent complete: 95.83608724388631%\n","Percent complete: 96.16655651024455%\n","Percent complete: 96.49702577660277%\n","Percent complete: 96.827495042961%\n","Percent complete: 97.15796430931924%\n","Percent complete: 97.48843357567746%\n","Percent complete: 97.8189028420357%\n","Percent complete: 98.14937210839392%\n","Percent complete: 98.47984137475216%\n","Percent complete: 98.81031064111038%\n","Percent complete: 99.1407799074686%\n","Percent complete: 99.47124917382683%\n","Percent complete: 99.80171844018506%\n","Scraping completed. Updated data saved to vessel_data_with_dates.csv.\n"]}]},{"cell_type":"markdown","source":["## SPRFMO\n","- [BG]: `Authorization Number for the South Pacific Regional Fisheries Management Organisation`\n","  - Use the *Registration Number* column\n","- [BH]: `Authorization Start Date for the South Pacific Regional Fisheries Management Organisation`\n","  - Use the *Authorised Date (Start)* field from the specific vessel page, e.g. https://sprfmo.org/vessels/2738\n","- [BI]: `Authorization End Date for the South Pacific Regional Fisheries Management Organisation`\n","  - Use the *Authorised Date (End)* column\n","-  [BJ]: `Vessel Authorization Link for the South Pacific Regional Fisheries Management Organisation`\n","  - Use the specific vessel page, found after clicking the search result, e.g. https://sprfmo.org/vessels/2738\n","\n","So for scraping purposes, we need to scrape the following from the base page results:\n","- For lookup in `master`\n","  - `Vessel Name`, `IMO Number`\n","- For new data\n","  - `Date Included in SPRFMO Record`, aka the \"Auth Start\" date when you visit an individual vessel page\n","  - the `ahref` behind the `Vessel Name` aka the authorization Link"],"metadata":{"id":"wgxioWtiWr1t"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","# URL of the website\n","base_url = \"https://sprfmo.org/vessels\"\n","MAX_QUERY = 2\n","\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.common.action_chains import ActionChains\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import time\n","\n","MAX_WAIT = 15\n","MAX_REQUESTS = 1000\n","\n","options.add_argument('--headless')\n","options.add_argument('--no-sandbox')\n","options.add_argument('--disable-dev-shm-usage')\n","driver = webdriver.Chrome(options=options)\n","\n","wait = WebDriverWait(driver, MAX_WAIT)\n","\n","\n","# URL of the website\n","url = \"https://sprfmo.org/vessels\"\n","\n","# Open the page\n","driver.get(url)\n","\n","# Function to scrape data from the current page\n","def scrape_page(soup):\n","    vessels = []\n","    rows = soup.find_all(\"div\", class_=\"row\")\n","\n","    for row in rows:\n","        try:\n","            name_label = row.find(\"label\", string=\"Vessel Name\")\n","            if name_label:\n","                vessel_name_div = name_label.find_next_sibling(\"div\")\n","                vessel_name_link = vessel_name_div.find(\"a\") if vessel_name_div else None\n","                vessel_name = vessel_name_link.text.strip() if vessel_name_link else \"N/A\"\n","                vessel_link = f\"{vessel_name_link['href']}\" if vessel_name_link else \"N/A\"\n","            else:\n","                vessel_name = \"N/A\"\n","                vessel_link = \"N/A\"\n","\n","            imo_number = row.find(\"div\", class_=\"imo-number-column\").div.text.strip()\n","            date_included = row.find(\"div\", class_=\"date-column\").div.text.strip()\n","\n","            imo_number = imo_number.split(\"IMO Number\")[1] if \"IMO Number\" in imo_number else imo_number\n","            date_included = date_included.split(\"Date Included in SPRFMO Record\")[1] if \"Date Included in SPRFMO Record\" in date_included else date_included\n","\n","            vessels.append({\n","                \"Vessel Name\": vessel_name,\n","                \"IMO Number\": imo_number,\n","                \"Vessel Link\": f\"https://sprfmo.org{vessel_link}\",\n","                \"Date Included\": date_included,\n","            })\n","        except AttributeError:\n","            # Skip rows that don't match the expected format\n","            continue\n","\n","    return vessels\n","\n","# Loop and click \"View more\" until all vessels are loaded\n","prev_row_count = 0\n","num_requests = 0\n","while True:\n","    try:\n","        # Wait for the \"View more\" button to be clickable\n","        wait = WebDriverWait(driver, 10)\n","        try:\n","          view_more_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[text()=\"View more\"]')))\n","\n","          # Click the button\n","          ActionChains(driver).move_to_element(view_more_button).click(view_more_button).perform()\n","        except Exception as e:\n","          print(f\"Error: {e}\")\n","\n","        num_requests += 1\n","\n","        # Wait for the new rows to load\n","        time.sleep(2)  # Adjust the delay if needed\n","\n","        # Scrape the new content\n","        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","        rows = soup.find_all(\"div\", class_=\"row\")\n","\n","        # Stop if no new rows are loaded\n","        if prev_row_count == len(rows) :\n","            print(\"No new rows detected, exiting.\")\n","            break\n","\n","        if (num_requests >= MAX_REQUESTS):\n","            print(\"Max requests reached, exiting.\")\n","            break\n","\n","        print (f\"Rows: {len(rows)}, request #: {num_requests}\")\n","        prev_row_count = len(rows)\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","        break\n","\n","# Now loop through page and extract desired data\n","data = []\n","soup = BeautifulSoup(driver.page_source, \"html.parser\")\n","data.extend(scrape_page(soup))\n","\n","# Convert the data to a DataFrame\n","df = pd.DataFrame(data)\n","\n","# Close the browser\n","driver.quit()\n","\n","# Save the DataFrame to a CSV file\n","df.to_csv(\"sprfmo_vessels-new.csv\", index=False)\n","\n"],"metadata":{"id":"IUR4TldiXp5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scrape_page(soup):\n","    vessels = []\n","    rows = soup.find_all(\"div\", class_=\"row\")\n","\n","    for row in rows:\n","        try:\n","            name_label = row.find(\"label\", string=\"Vessel Name\")\n","            if name_label:\n","                vessel_name_div = name_label.find_next_sibling(\"div\")\n","                vessel_name_link = vessel_name_div.find(\"a\") if vessel_name_div else None\n","                vessel_name = vessel_name_link.text.strip() if vessel_name_link else \"N/A\"\n","                vessel_link = f\"{vessel_name_link['href']}\" if vessel_name_link else \"\"\n","            else:\n","                vessel_name = \"N/A\"\n","                vessel_link = \"N/A\"\n","\n","            imo_label = row.find(\"label\", string=\"IMO Number\")\n","            if imo_label:\n","                imo_number_div = imo_label.find_next_sibling(\"div\")\n","                imo_number = imo_number_div.text.strip()\n","\n","            date_label = row.find(\"label\", string=\"Date Included in SPRFMO Record\")\n","            if date_label:\n","                date_included_div = date_label.find_next_sibling(\"div\")\n","                date_included = date_included_div.text.strip()\n","\n","            vessels.append({\n","                \"Vessel Name\": vessel_name,\n","                \"IMO Number\": imo_number,\n","                \"Vessel Link\": f\"https://sprfmo.org{vessel_link}\",\n","                \"Date Included\": date_included,\n","            })\n","        except AttributeError:\n","            # Skip rows that don't match the expected format\n","            continue\n","\n","    return vessels\n","\n","data = []\n","data.extend(scrape_page(soup))\n","\n","df = pd.DataFrame(data)\n","df.to_csv(\"plz-work.csv\", index=False)"],"metadata":{"id":"ZDloKZ0GtCPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_deduped = df.drop_duplicates(subset=['IMO Number', 'Vessel Name'])\n","# get rid of 'n/a' rows\n","df_deduped = df_deduped[df_deduped['Vessel Name'] != 'N/A']\n","df_deduped.to_csv(\"plz-work-duped.csv\", index=False)"],"metadata":{"id":"ikAgMjjPhMjL"},"execution_count":null,"outputs":[]}]}